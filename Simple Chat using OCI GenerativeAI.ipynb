{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c6f302",
   "metadata": {},
   "source": [
    "# Simple Chat using OCI Generative AI\n",
    "\n",
    "The notebook sets up a simple chat application using Oracle Cloud Infrastructure (OCI) Generative AI services. Here's an overview of the entire notebook:\n",
    "\n",
    "1. Imports and Environment Setup:\n",
    "\n",
    "    - Imports necessary libraries such as os, ads, oracledb, dotenv, and oci_genai.\n",
    "    - Loads environment variables from a .env file using load_dotenv().\n",
    "\n",
    "2. Embeddings Initialization:\n",
    "\n",
    "    - Creates an instance of OCIGenAIEmbeddings using environment variables for model ID, service endpoint, and compartment ID.\n",
    "\n",
    "3. Database Connection:\n",
    "\n",
    "    - Establishes a connection to an Oracle database using oracledb.connect() with credentials retrieved from environment variables.\n",
    "\n",
    "4. Vector Store Setup:\n",
    "\n",
    "    - Defines the table name \"DOCS\" for storing vectors.\n",
    "    - Initializes the OracleVS vector store with the database connection, embeddings, and table name.\n",
    "\n",
    "5. Retriever Setup:\n",
    "\n",
    "    - Converts the vector store into a retriever using vector_store.as_retriever().\n",
    "\n",
    "6. Prompt Template Definition:\n",
    "\n",
    "    - Defines a prompt template for the RAG model to use when generating answers.\n",
    "    - The template instructs the assistant to use retrieved context fragments to answer questions.\n",
    "\n",
    "7. Chat Model Initialization:\n",
    "\n",
    "    - Creates an instance of ChatOCIGenAI with specified parameters, including model ID, service endpoint, compartment ID, and other model settings.\n",
    "\n",
    "8. RAG Model Setup:\n",
    "\n",
    "    - Combines the chat model and retriever into a RetrievalQA model using RetrievalQA.from_chain_type().\n",
    "    - Uses the defined prompt template for generating answers.\n",
    "\n",
    "9. Example Usage:\n",
    "\n",
    "    - Demonstrates how to use the RAG model to generate responses to user inputs.\n",
    "    - Sends a prompt to the model and prints the generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b44c7f-4e95-48c7-a2ef-8ef502d49ccf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T18:40:06.993545Z",
     "iopub.status.busy": "2025-01-23T18:40:06.993228Z",
     "iopub.status.idle": "2025-01-23T18:40:10.409534Z",
     "shell.execute_reply": "2025-01-23T18:40:10.407946Z",
     "shell.execute_reply.started": "2025-01-23T18:40:06.993521Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ads\n",
    "import oracledb\n",
    "from dotenv import load_dotenv\n",
    "import oci_genai\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36b810f7-85e6-4cc4-a6e3-cfcb077e8547",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T18:46:19.787233Z",
     "iopub.status.busy": "2025-01-23T18:46:19.786583Z",
     "iopub.status.idle": "2025-01-23T18:46:22.852593Z",
     "shell.execute_reply": "2025-01-23T18:46:22.851250Z",
     "shell.execute_reply.started": "2025-01-23T18:46:19.787213Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import ads\n",
    "import oracledb\n",
    "from dotenv import load_dotenv\n",
    "import oci_genai\n",
    "from langchain_community.chat_models.oci_generative_ai import ChatOCIGenAI\n",
    "from langchain_community.embeddings.oci_generative_ai import OCIGenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import OracleVS\n",
    "import oracledb\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embeddings = OCIGenAIEmbeddings(\n",
    "    model_id=os.getenv('CON_GEN_AI_EMB_MODEL_ID'),\n",
    "    service_endpoint=os.getenv('CON_GEN_AI_SERVICE_ENDPOINT'),\n",
    "    compartment_id=os.getenv('CON_GEN_AI_COMPARTMENT_ID'),\n",
    ")\n",
    "\n",
    "\n",
    "connection = oracledb.connect(\n",
    "    user=os.getenv('CON_ADB_DEV_USER_NAME'), \n",
    "    password=os.getenv('CON_ADB_DEV_PASSWORD'), \n",
    "    dsn=os.getenv('CON_ADB_DEV_SERVICE_NAME')\n",
    ")\n",
    "\n",
    "\n",
    "#vector store table name\n",
    "table_name = \"DOCS\"\n",
    "\n",
    "vector_store = OracleVS(connection, embeddings, table_name)\n",
    "\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    # search_kwargs={ 'k': 100} <- parameter to define the number of documents to retrieve\n",
    ")\n",
    "\n",
    "rag_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "            Please use only the following retrieved context fragments to answer the question.\n",
    "            If you don't know the answer, say you the cuestion is out of my context.\n",
    "            Always use all available data.:\n",
    "            {context}\n",
    "            Question: {question}\n",
    "            \"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(rag_prompt_template)\n",
    "\n",
    "\n",
    "llm = ChatOCIGenAI(\n",
    "        model_id         = os.getenv(\"CON_GEN_AI_CHAT_MODEL_ID\"),\n",
    "        service_endpoint = os.getenv(\"CON_GEN_AI_SERVICE_ENDPOINT\"),\n",
    "        compartment_id   = os.getenv(\"CON_GEN_AI_COMPARTMENT_ID\"),\n",
    "        provider         = \"meta\",\n",
    "        is_stream        = True,\n",
    "        auth_type        = os.getenv(\"CON_GEN_AI_AUTH_TYPE\"),\n",
    "        model_kwargs     = {\n",
    "            \"max_tokens\"        : 1024,\n",
    "            \"temperature\"       : 0.6,\n",
    "            \"top_p\"             : 0.7,\n",
    "            \"top_k\"             : 20,\n",
    "            \"frequency_penalty\" : 0\n",
    "        }\n",
    "    )\n",
    "\n",
    "rag = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d107690-b64b-4e95-b5be-5ede7ed13b64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T18:46:34.506934Z",
     "iopub.status.busy": "2025-01-23T18:46:34.506620Z",
     "iopub.status.idle": "2025-01-23T18:47:27.066048Z",
     "shell.execute_reply": "2025-01-23T18:47:27.064558Z",
     "shell.execute_reply.started": "2025-01-23T18:46:34.506917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what RAG mean and what do I need to create one?', 'result': \"RAG stands for Retrieval Augmented Generation. It is a type of AI model that combines the strengths of information retrieval and natural language generation to produce factually accurate and contextually relevant responses.\\n\\nTo create a RAG system, you need to follow these key steps:\\n\\n1. Data Collection: Collect relevant, domain-specific textual data from various external sources, such as PDFs, structured documents, or text files.\\n2. Data Preprocessing: Preprocess the collected data to create manageable and meaningful chunks. This involves cleaning the text, removing noise, and normalizing the text format.\\n3. Vectorization: Convert the preprocessed text into vector embeddings using transformer-based models like BERT or Sentence Transformers.\\n4. Indexing: Store the vector embeddings in a vector database optimized for fast similarity-based retrieval.\\n5. Retrieval: Use a retriever to convert user queries into vector embeddings and search the vector database for relevant content.\\n6. Generation: Use a generator to synthesize the retrieved content into a coherent, factual response.\\n\\nYou can use various tools and frameworks to develop a RAG system, including OpenAI's Assistant API, Llama, and FAISS. The choice of tool depends on your specific requirements, such as ease of use, customization, and cost.\\n\\nAdditionally, you may need to consider the following:\\n\\n* Handling complex PDF layouts and extracting useful text\\n* Utilizing PDF metadata and annotations\\n* Implementing error handling and logging\\n* Normalizing text and utilizing annotations\\n* Fine-tuning the model for specific tasks or domains\\n\\nIt's also important to note that RAG systems require complex infrastructure, including vector databases and effective retrieval pipelines, and can be resource-intensive during inference.\"}\n"
     ]
    }
   ],
   "source": [
    "print(rag.invoke(\"what RAG mean and what do I need to create one?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438c7a0b-c1ae-461e-ae1f-f3d8b8b2c3b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T18:41:34.902620Z",
     "iopub.status.busy": "2025-01-23T18:41:34.901873Z",
     "iopub.status.idle": "2025-01-23T18:41:43.655543Z",
     "shell.execute_reply": "2025-01-23T18:41:43.653841Z",
     "shell.execute_reply.started": "2025-01-23T18:41:34.902597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what RAG mean and what do I need to create one?', 'result': 'RAG stands for Retrieval Augmented Generation. It is a model that combines Large Language Models (LLMs) with a retrieval mechanism, allowing it to access external data sources in real-time. This makes it suitable for scenarios requiring up-to-date or frequently changing information.\\n\\nTo create a RAG model, you would need to integrate two core components of NLP: Information Retrieval (IR) and Natural Language Processing (NLP). Additionally, RAG requires complex infrastructure, including vector databases, to function effectively.'}\n"
     ]
    }
   ],
   "source": [
    "print(rag.invoke(\"what RAG mean and what do I need to create one?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a11de710-3994-4caa-9a66-6964f379e081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T18:41:47.191013Z",
     "iopub.status.busy": "2025-01-23T18:41:47.190737Z",
     "iopub.status.idle": "2025-01-23T18:41:49.403515Z",
     "shell.execute_reply": "2025-01-23T18:41:49.402482Z",
     "shell.execute_reply.started": "2025-01-23T18:41:47.190996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Can you talk about peruvian food?', 'result': 'The question is out of my context.'}\n"
     ]
    }
   ],
   "source": [
    "print(rag.invoke(\"Can you talk about peruvian food?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168cf163-bcc1-4eb9-a64b-74429b5b273e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3c974-965c-41f8-883a-94c5f0b9076b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
