{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader, CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models.oci_generative_ai import ChatOCIGenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OCIGenAIEmbeddings\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, END, MessagesState, StateGraph\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Step 1: Load PDF and CSV Documents\n",
    "def load_documents():\n",
    "    # Load PDF Document\n",
    "    pdf_loader = PyPDFLoader(\"demo.pdf\")\n",
    "    pdf_documents = pdf_loader.load()\n",
    "    return pdf_documents\n",
    " \n",
    "# Step 2: Split the documents into smaller chunks for better processing\n",
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    return split_docs\n",
    "\n",
    "# Load and process documents\n",
    "documents = load_documents()\n",
    "chunks = split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = OCIGenAIEmbeddings(\n",
    "    model_id=os.getenv(\"CON_GEN_AI_EMB_MODEL_ID\"),\n",
    "    service_endpoint=os.getenv(\"CON_GEN_AI_SERVICE_ENDPOINT\"),\n",
    "    compartment_id=os.getenv(\"CON_GEN_AI_COMPARTMENT_ID\"),\n",
    "    auth_type=\"API_KEY\",\n",
    "    model_kwargs={\"input_type\": \"SEARCH_DOCUMENT\"}\n",
    ")\n",
    "\n",
    "db = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 50, 'fetch_k': 150}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1a1ac0eb0e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOCIGenAI(\n",
    "            model_id=os.getenv(\"CON_GEN_AI_CHAT_MODEL_ID\"),\n",
    "            service_endpoint=os.getenv(\"CON_GEN_AI_SERVICE_ENDPOINT\"),\n",
    "            compartment_id=os.getenv(\"CON_GEN_AI_COMPARTMENT_ID\"),\n",
    "            provider=\"meta\",\n",
    "            is_stream=True,\n",
    "            auth_type=os.getenv(\"CON_GEN_AI_AUTH_TYPE\"),\n",
    "            model_kwargs={\n",
    "                \"max_tokens\": 1024,\n",
    "                \"temperature\": 0.6,\n",
    "                \"top_p\": 0.7,\n",
    "                \"top_k\": 20,\n",
    "                \"frequency_penalty\": 0,\n",
    "            },\n",
    "        )\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Define the retriever function\n",
    "def retrieve_relevant_documents(state: MessagesState):\n",
    "    query = state[\"messages\"][-1].content  # Extract last message as query\n",
    "    retrieved_docs = retriever.invoke(query)  # Assuming retriever is defined\n",
    "    retrieved_content = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    state[\"messages\"].append(SystemMessage(content=f\"Context: {retrieved_content}\"))\n",
    "    # print(\"state en el retrieve\", state)\n",
    "    return state  # Return updated state\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant that call me by my name if posible. \"\n",
    "        \"Use the following retrieved documents as context to provide accurate responses. \"\n",
    "        \"Answer all questions to the best of your ability and if the answer is not in the context, respond with 'The question is out of my context.' .\"\n",
    "    )\n",
    "\n",
    "    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
    "    response = chat.invoke(messages)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define the nodes and edges\n",
    "workflow.add_node(\"retriever\", retrieve_relevant_documents)\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"retriever\")\n",
    "workflow.add_edge(\"retriever\", \"model\")\n",
    "workflow.add_edge(\"model\", END)  # Add end node\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hola Evelyn, \n",
      "\n",
      "The top risks mentioned in the document include:\n",
      "\n",
      "1. Dependence on advertising revenue: The company generates a significant portion of its revenue from advertising, and any decline in advertising spending or loss of partners could harm its business.\n",
      "\n",
      "2. Competition: The company faces intense competition in the technology industry, which could lead to a loss of users, advertisers, and revenue.\n",
      "\n",
      "3. Regulatory risks: The company is subject to various laws and regulations, including those related to data protection, intellectual property, and antitrust, and any failure to comply with these laws could result in significant liabilities and penalties.\n",
      "\n",
      "4. Cybersecurity risks: The company is vulnerable to cyber attacks, which could compromise user data and harm its business.\n",
      "\n",
      "5. International risks: The company's international operations expose it to additional risks, including restrictions on foreign ownership and investments, import and export requirements, and fluctuations in foreign currency exchange rates.\n",
      "\n",
      "6. Intellectual property risks: The company relies on intellectual property laws, confidentiality procedures, and contractual provisions to protect its proprietary technology and brand, but these measures may not be sufficient to prevent intellectual property infringement.\n",
      "\n",
      "7. Supply chain risks: The company relies on other companies to manufacture its products and provide services, and any disruption to these supply chains could harm its business.\n",
      "\n",
      "8. Economic risks: The company is exposed to fluctuations in the global economic climate and financial markets, which could result in a significant impairment charge relating to its non-marketable equity securities.\n",
      "\n",
      "9. Data protection risks: The company is subject to various data protection laws and regulations, including the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA), and any failure to comply with these laws could result in significant liabilities and penalties.\n",
      "\n",
      "10. Reputation risks: The company's reputation is critical to its business, and any negative publicity or failure to maintain its corporate culture and values could harm its reputation and business.\n",
      "\n",
      "These are some of the top risks mentioned in the document, but there may be other risks that are not mentioned or that are not as prominent.\n"
     ]
    }
   ],
   "source": [
    "def translate_chat_history(lista):\n",
    "    translated_history = []    \n",
    "    for item in lista:\n",
    "        role = item['role'].lower()\n",
    "        message = item['message']\n",
    "        if role == 'human':\n",
    "            translated_history.append(HumanMessage(content=message))\n",
    "        elif role == 'assistant':\n",
    "            translated_history.append(AIMessage(content=message))    \n",
    "    return translated_history\n",
    "\n",
    "# Example usage:\n",
    "import json\n",
    "body = {\n",
    "    \"p_query\":\"what are the top risks mentioned in the document?\",\n",
    "    \"p_role\" :\"private\",\n",
    "    \"p_history\": \"[{\\\"role\\\":\\\"Assistant\\\",\\\"message\\\":\\\"Hello, what is your name?\\\"},\\\n",
    "    {\\\"role\\\":\\\"Human\\\",\\\"message\\\":\\\"Hello, my name is Evelyn\\\"}]\"\n",
    "}\n",
    "p_history = json.loads(body['p_history'])\n",
    "chat_history = translate_chat_history(p_history)\n",
    "\n",
    "# Add simple in-memory checkpointer\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "response  = app.invoke(\n",
    "    {\n",
    "        \"messages\": chat_history\n",
    "        + [HumanMessage(content=body[\"p_query\"])]\n",
    "    },\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")\n",
    "response[\"messages\"][-1].pretty_print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
