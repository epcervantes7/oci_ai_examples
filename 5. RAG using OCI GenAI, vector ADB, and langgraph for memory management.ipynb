{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader, CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models.oci_generative_ai import ChatOCIGenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OCIGenAIEmbeddings\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, END, MessagesState, StateGraph\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Step 1: Load PDF and CSV Documents\n",
    "def load_documents():\n",
    "    # Load PDF Document\n",
    "    pdf_loader = PyPDFLoader(\"demo.pdf\")\n",
    "    pdf_documents = pdf_loader.load()\n",
    "    return pdf_documents\n",
    " \n",
    "# Step 2: Split the documents into smaller chunks for better processing\n",
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    return split_docs\n",
    "\n",
    "# Load and process documents\n",
    "documents = load_documents()\n",
    "chunks = split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import oracledb\n",
    "from typing import List, Dict\n",
    "from langchain_core.messages import BaseMessage, AIMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import OracleVS\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import  MessagesPlaceholder\n",
    "from langchain.chains import (\n",
    "    create_history_aware_retriever,\n",
    "    create_retrieval_chain\n",
    ")\n",
    "from langchain_community.chat_models import ChatOCIGenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.embeddings import OCIGenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# embeddings = OCIGenAIEmbeddings(\n",
    "#     model_id=os.getenv(\"CON_GEN_AI_EMB_MODEL_ID\"),\n",
    "#     service_endpoint=os.getenv(\"CON_GEN_AI_SERVICE_ENDPOINT\"),\n",
    "#     compartment_id=os.getenv(\"CON_GEN_AI_COMPARTMENT_ID\"),\n",
    "#     auth_type=\"API_KEY\",\n",
    "#     model_kwargs={\"input_type\": \"SEARCH_DOCUMENT\"}\n",
    "# )\n",
    "\n",
    "# db = FAISS.from_documents(chunks, embeddings)\n",
    "# retriever = db.as_retriever(\n",
    "#     search_type=\"mmr\",\n",
    "#     search_kwargs={'k': 50, 'fetch_k': 150}\n",
    "# )\n",
    "\n",
    "default_path=\"\"\n",
    "CONFIG_PROFILE = \"DEFAULT\"\n",
    "llm = ChatOCIGenAI(\n",
    "        model_id         = os.getenv(\"CON_GEN_AI_CHAT_MODEL_ID\"),\n",
    "        service_endpoint = os.getenv(\"CON_GEN_AI_SERVICE_ENDPOINT\"),\n",
    "        compartment_id   = os.getenv(\"CON_GEN_AI_COMPARTMENT_ID\"),\n",
    "        provider         = \"meta\",\n",
    "        is_stream        = True,\n",
    "        auth_type        = os.getenv(\"CON_GEN_AI_AUTH_TYPE\"),\n",
    "        auth_file_location=default_path+\"oci/config\",\n",
    "        auth_profile=CONFIG_PROFILE,\n",
    "        model_kwargs     = {\n",
    "            \"max_tokens\"        : 1024,\n",
    "            \"temperature\"       : 0.6,\n",
    "            \"top_p\"             : 0.7,\n",
    "            \"top_k\"             : 20,\n",
    "            \"frequency_penalty\" : 0\n",
    "        }        \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "embeddings = OCIGenAIEmbeddings(\n",
    "            model_id=os.getenv('CON_GEN_AI_EMB_MODEL_ID'),\n",
    "            service_endpoint=os.getenv('CON_GEN_AI_SERVICE_ENDPOINT'),\n",
    "            compartment_id=os.getenv('CON_GEN_AI_COMPARTMENT_ID'),\n",
    "            truncate=\"NONE\",\n",
    "            auth_file_location=default_path+\"oci/config\",\n",
    "            auth_type=os.getenv(\"CON_GEN_AI_AUTH_TYPE\"),\n",
    "            auth_profile=CONFIG_PROFILE\n",
    "        )\n",
    "\n",
    "default_path = \"\"\n",
    "connection = oracledb.connect(\n",
    "    user=os.getenv('CON_ADB_DEV_USER_NAME'), \n",
    "    password=os.getenv('CON_ADB_DEV_PASSWORD'), \n",
    "    dsn=os.getenv('CON_ADB_DEV_SERVICE_NAME'),\n",
    "    config_dir=default_path+\"oci\",\n",
    "    wallet_location=default_path+\"oci\",\n",
    "    wallet_password=os.getenv('DB_WALLET_PASSWORD')\n",
    "    )\n",
    "\n",
    "table_name = \"MY_DOCS\"\n",
    "vector_store = OracleVS(connection, embeddings, table_name)\n",
    "retriever = vector_store.as_retriever(\n",
    "    # search_kwargs={ 'k': 100} <- parameter to define the number of documents to retrieve\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x18ec73027b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chat = ChatOCIGenAI(\n",
    "#             model_id=os.getenv(\"CON_GEN_AI_CHAT_MODEL_ID\"),\n",
    "#             service_endpoint=os.getenv(\"CON_GEN_AI_SERVICE_ENDPOINT\"),\n",
    "#             compartment_id=os.getenv(\"CON_GEN_AI_COMPARTMENT_ID\"),\n",
    "#             provider=\"meta\",\n",
    "#             is_stream=True,\n",
    "#             auth_type=os.getenv(\"CON_GEN_AI_AUTH_TYPE\"),\n",
    "#             model_kwargs={\n",
    "#                 \"max_tokens\": 1024,\n",
    "#                 \"temperature\": 0.6,\n",
    "#                 \"top_p\": 0.7,\n",
    "#                 \"top_k\": 20,\n",
    "#                 \"frequency_penalty\": 0,\n",
    "#             },\n",
    "#         )\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Define the retriever function\n",
    "def retrieve_relevant_documents(state: MessagesState):\n",
    "    query = state[\"messages\"][-1].content  # Extract last message as query\n",
    "    retrieved_docs = retriever.invoke(query)  # Assuming retriever is defined\n",
    "    retrieved_content = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    state[\"messages\"].append(SystemMessage(content=f\"Context: {retrieved_content}\"))\n",
    "    # print(\"state en el retrieve\", state)\n",
    "    return state  # Return updated state\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant that call me by my name if posible. \"\n",
    "        \"Use the following retrieved documents as context to provide accurate responses. \"\n",
    "        \"Answer all questions to the best of your ability and if the answer is not in the context, respond with 'The question is out of my context.' .\"\n",
    "    )\n",
    "\n",
    "    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define the nodes and edges\n",
    "workflow.add_node(\"retriever\", retrieve_relevant_documents)\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"retriever\")\n",
    "workflow.add_edge(\"retriever\", \"model\")\n",
    "workflow.add_edge(\"model\", END)  # Add end node\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Oracle AI Vector Search is designed for Artificial Intelligence (AI) workloads and allows you to query data based on semantics, rather than keywords. It stores vector embeddings, which are mathematical vector representations of data points, and these vector embeddings describe the semantic meaning behind content such as words, documents, audio tracks, or images.\n"
     ]
    }
   ],
   "source": [
    "def translate_chat_history(lista):\n",
    "    translated_history = []    \n",
    "    for item in lista:\n",
    "        role = item['role'].lower()\n",
    "        message = item['message']\n",
    "        if role == 'human':\n",
    "            translated_history.append(HumanMessage(content=message))\n",
    "        elif role == 'assistant':\n",
    "            translated_history.append(AIMessage(content=message))    \n",
    "    return translated_history\n",
    "\n",
    "# Example usage:\n",
    "import json\n",
    "# body = {\n",
    "#     \"p_query\":\"what are the top risks mentioned in the document?\",\n",
    "#     \"p_role\" :\"private\",\n",
    "#     \"p_history\": \"[{\\\"role\\\":\\\"Assistant\\\",\\\"message\\\":\\\"Hello, what is your name?\\\"},\\\n",
    "#     {\\\"role\\\":\\\"Human\\\",\\\"message\\\":\\\"Hello, my name is Evelyn\\\"}]\"\n",
    "# }\n",
    "data = {\n",
    "    \"query\": \"what is Oracle AI Vector Search?\",\n",
    "    # \"query\": \"in what applications can I use it?\",\n",
    "    \"context\": [\n",
    "       {\n",
    "            \"role\": \"user\",\n",
    "            \"message\": \"what is Oracle AI Vector Search?\"\n",
    "        },\n",
    "        {\"role\": \"assistant\",\n",
    "            \"message\": \"Oracle AI Vector Search is designed for Artificial Intelligence (AI) workloads and allows you to query data based on semantics, rather than keywords. It stores vector embeddings, which are mathematical vector representations of data points, and these vector embeddings describe the semantic meaning behind content such as words, documents, audio tracks, or images.\"}\n",
    "]\n",
    "}\n",
    "body = data\n",
    "# p_history = json.loads(body['context'])\n",
    "chat_history = translate_chat_history(body['context'])\n",
    "\n",
    "# Add simple in-memory checkpointer\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "response  = app.invoke(\n",
    "    {\n",
    "        \"messages\": chat_history\n",
    "        + [HumanMessage(content=body[\"query\"])]\n",
    "    },\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")\n",
    "response[\"messages\"][-1].pretty_print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
